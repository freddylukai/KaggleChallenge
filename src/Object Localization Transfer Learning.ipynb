{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Object Detection API\n",
    "\n",
    "This approach uses transfer learning from the `faster_rcnn_resnet50_coco` (check the tf [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)), which is pretrained with the [COCO dataset](http://cocodataset.org).\n",
    "\n",
    "- To install Tensorflow Object Detection API clone the [repo](https://github.com/tensorflow/models/) (we'll use code from the `research/object_detection` folder) and follow [these](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) installation instructions.\n",
    "- I've used Windows (sort of a pain) but is possible. Some of the instructions to install the API in windows (prior to this I had an anaconda environment with python 3 and tensorflow-gpu configured): https://medium.com/@rohitrpatil/how-to-use-tensorflow-object-detection-api-on-windows-102ec8097699.\n",
    "\n",
    "- This is based heavily on this blog post: https://medium.com/practical-deep-learning/a-complete-transfer-learning-toolchain-for-semantic-segmentation-3892d722b604. Code [here](https://github.com/fera0013/TransferLearningToolchain).\n",
    "\n",
    "- This blog post was also useful: https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9.\n",
    "\n",
    "\n",
    "This notebook used the [faster_rcnn_resnet50_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_coco_2018_01_28.tar.gz) model. The model binaries are not commited. To run this notebook you will have to download the model, unzip it and copy the following files to the `src/model/faster_rcnn_resnet50_coco_2018_01_28` folder:\n",
    "\n",
    "- `model.ckpt.data-00000-of-00001`\n",
    "- `model.ckpt.index`\n",
    "- `model.ckpt.meta`\n",
    "\n",
    "TODO: Automate this process within the notebook :-) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydicom'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-43896000266e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpylab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpydicom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydicom'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "# This is the path of the tf models repo. \n",
    "# I had to do this since my Anaconda installation on windows didn't use\n",
    "# the PYTHONPATH. If your python environment uses PYTHONPATH add these\n",
    "# paths to it (using the appropiate models path).\n",
    "sys.path.append('C:\\\\Users\\\\bones\\\\models')\n",
    "sys.path.append('C:\\\\Users\\\\bones\\\\models\\\\research')\n",
    "sys.path.append('C:\\\\Users\\\\bones\\\\models\\\\research\\\\slim')\n",
    "\n",
    "\n",
    "import glob, pylab, pandas as pd\n",
    "import pydicom, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, OrderedDict\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.legacy import evaluator\n",
    "from offline_eval_map_corloc import _generate_filenames\n",
    "from object_detection.metrics import tf_example_parser\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.legacy import trainer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data to TFRecords\n",
    "\n",
    "o use the object detection API is neccesary to convert the data to TFrecords, a somewhat obscure tensorflow format. Is very useful to read this to understand it first: https://planspace.org/20170323-tfrecords_for_humans/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Should we move this to a common file?\n",
    "def parse_data(df):\n",
    "    \"\"\"\n",
    "    Method to read a CSV file (Pandas dataframe) and parse the \n",
    "    data into the following nested dictionary:\n",
    "\n",
    "      parsed = {\n",
    "        \n",
    "        'patientId-00': {\n",
    "            'dicom': path/to/dicom/file,\n",
    "            'label': either 0 or 1 for normal or pnuemonia, \n",
    "            'boxes': list of box(es)\n",
    "        },\n",
    "        'patientId-01': {\n",
    "            'dicom': path/to/dicom/file,\n",
    "            'label': either 0 or 1 for normal or pnuemonia, \n",
    "            'boxes': list of box(es)\n",
    "        }, ...\n",
    "\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Define lambda to extract coords in list [y, x, height, width]\n",
    "    extract_box = lambda row: [row['y'], row['x'], row['height'], row['width']]\n",
    "\n",
    "    parsed = {}\n",
    "    for n, row in df.iterrows():\n",
    "        # --- Initialize patient entry into parsed \n",
    "        pid = row['patientId']\n",
    "        if pid not in parsed:\n",
    "            parsed[pid] = {\n",
    "                'dicom': '../input/stage_1_train_images/%s.dcm' % pid,\n",
    "                'label': row['Target'],\n",
    "                'boxes': []}\n",
    "\n",
    "        # --- Add box if opacity is present\n",
    "        if parsed[pid]['label'] == 1:\n",
    "            parsed[pid]['boxes'].append(extract_box(row))\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    data -> a data entry generated by parse_data\n",
    "    filename -> the image filename that will be stored (inside the tf record). \n",
    "                This must have a jpg extension since we're encoding to jpg in the tfrecord.\n",
    "'''\n",
    "def create_tf_example(data, filename):\n",
    "    dcm_data = pydicom.read_file(data['dicom'])\n",
    "    encoded_jpg = cv2.imencode('.jpg', dcm_data.pixel_array)[1].tostring()\n",
    "    \n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for box in data['boxes']:\n",
    "        xmin, ymin, w, h = box\n",
    "        xmax, ymax = xmin + w, ymin + h\n",
    "        xmins.append(xmin / width)\n",
    "        xmaxs.append(xmax / width)\n",
    "        ymins.append(ymin / height)\n",
    "        ymaxs.append(ymax / height)\n",
    "        \n",
    "        # We only have one class..\n",
    "        classes_text.append('pneumonia'.encode('utf8'))\n",
    "        classes.append(1)\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    \n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    images_metadata -> dictionary generated by parse_data\n",
    "    base_path -> the path of the folder where the records will be stored.\n",
    "    filename -> the name of the .record file\n",
    "'''\n",
    "def create_tf_records(images_metadata, base_path, filename, force=False):\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        os.mkdir(base_path)\n",
    "    \n",
    "    output_path = os.path.join(base_path, filename)\n",
    "    if os.path.isfile(output_path):\n",
    "        print(\"%s already exists\" % output_path)\n",
    "        \n",
    "        if force is True:\n",
    "            print(\"Skipping creation of the Tfrecord\")\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "            \n",
    "    else:\n",
    "        print(\"%s does not exist. Proceeding to create\" % output_path)\n",
    "        \n",
    "    with tf.python_io.TFRecordWriter(output_path) as writer:\n",
    "        for idx, patientId in enumerate(images_metadata):\n",
    "            tf_example = create_tf_example(images_metadata[patientId], '%s.jpg' % patientId)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(\"%d proccessed so far\" % (idx + 1))\n",
    "            \n",
    "        print(\"Created in %s\" % output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data and splitting into train and test. Both datasets will be stored in different tfrecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df shape (19422, 6)\n",
      "Test df shape (9567, 6)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/stage_1_train_labels.csv')\n",
    "train_df, test_df = train_test_split(df, test_size=0.33, random_state=42)\n",
    "print(\"Train df shape\", train_df.shape)\n",
    "print(\"Test df shape\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to create the TFRecords. This will create two files `validation.record` and `train.record` under the `input/stage_1_train_images/tf_records/` directory. Note that this can take some time (~30 min in my laptop), but this must be run only once.\n",
    "\n",
    "If by whatever reason you need to override existing records, set `force=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/stage_1_train_images/tf_records/validation.record does not exist. Proceeding to create\n",
      "100 proccessed so far\n",
      "200 proccessed so far\n",
      "300 proccessed so far\n",
      "400 proccessed so far\n",
      "500 proccessed so far\n",
      "600 proccessed so far\n",
      "700 proccessed so far\n",
      "800 proccessed so far\n",
      "900 proccessed so far\n",
      "1000 proccessed so far\n",
      "1100 proccessed so far\n",
      "1200 proccessed so far\n",
      "1300 proccessed so far\n",
      "1400 proccessed so far\n",
      "1500 proccessed so far\n",
      "1600 proccessed so far\n",
      "1700 proccessed so far\n",
      "1800 proccessed so far\n",
      "1900 proccessed so far\n",
      "2000 proccessed so far\n",
      "2100 proccessed so far\n",
      "2200 proccessed so far\n",
      "2300 proccessed so far\n",
      "2400 proccessed so far\n",
      "2500 proccessed so far\n",
      "2600 proccessed so far\n",
      "2700 proccessed so far\n",
      "2800 proccessed so far\n",
      "2900 proccessed so far\n",
      "3000 proccessed so far\n",
      "3100 proccessed so far\n",
      "3200 proccessed so far\n",
      "3300 proccessed so far\n",
      "3400 proccessed so far\n",
      "3500 proccessed so far\n",
      "3600 proccessed so far\n",
      "3700 proccessed so far\n",
      "3800 proccessed so far\n",
      "3900 proccessed so far\n",
      "4000 proccessed so far\n",
      "4100 proccessed so far\n",
      "4200 proccessed so far\n",
      "4300 proccessed so far\n",
      "4400 proccessed so far\n",
      "4500 proccessed so far\n",
      "4600 proccessed so far\n",
      "4700 proccessed so far\n",
      "4800 proccessed so far\n",
      "4900 proccessed so far\n",
      "5000 proccessed so far\n",
      "5100 proccessed so far\n",
      "5200 proccessed so far\n",
      "5300 proccessed so far\n",
      "5400 proccessed so far\n",
      "5500 proccessed so far\n",
      "5600 proccessed so far\n",
      "5700 proccessed so far\n",
      "5800 proccessed so far\n",
      "5900 proccessed so far\n",
      "6000 proccessed so far\n",
      "6100 proccessed so far\n",
      "6200 proccessed so far\n",
      "6300 proccessed so far\n",
      "6400 proccessed so far\n",
      "6500 proccessed so far\n",
      "6600 proccessed so far\n",
      "6700 proccessed so far\n",
      "6800 proccessed so far\n",
      "6900 proccessed so far\n",
      "7000 proccessed so far\n",
      "7100 proccessed so far\n",
      "7200 proccessed so far\n",
      "7300 proccessed so far\n",
      "7400 proccessed so far\n",
      "7500 proccessed so far\n",
      "7600 proccessed so far\n",
      "7700 proccessed so far\n",
      "7800 proccessed so far\n",
      "7900 proccessed so far\n",
      "8000 proccessed so far\n",
      "8100 proccessed so far\n",
      "8200 proccessed so far\n",
      "8300 proccessed so far\n",
      "8400 proccessed so far\n",
      "8500 proccessed so far\n",
      "8600 proccessed so far\n",
      "8700 proccessed so far\n",
      "8800 proccessed so far\n",
      "8900 proccessed so far\n",
      "9000 proccessed so far\n",
      "9100 proccessed so far\n",
      "Created in ../input/stage_1_train_images/tf_records/validation.record\n",
      "../input/stage_1_train_images/tf_records/train.record does not exist. Proceeding to create\n",
      "100 proccessed so far\n",
      "200 proccessed so far\n",
      "300 proccessed so far\n",
      "400 proccessed so far\n",
      "500 proccessed so far\n",
      "600 proccessed so far\n",
      "700 proccessed so far\n",
      "800 proccessed so far\n",
      "900 proccessed so far\n",
      "1000 proccessed so far\n",
      "1100 proccessed so far\n",
      "1200 proccessed so far\n",
      "1300 proccessed so far\n",
      "1400 proccessed so far\n",
      "1500 proccessed so far\n",
      "1600 proccessed so far\n",
      "1700 proccessed so far\n",
      "1800 proccessed so far\n",
      "1900 proccessed so far\n",
      "2000 proccessed so far\n",
      "2100 proccessed so far\n",
      "2200 proccessed so far\n",
      "2300 proccessed so far\n",
      "2400 proccessed so far\n",
      "2500 proccessed so far\n",
      "2600 proccessed so far\n",
      "2700 proccessed so far\n",
      "2800 proccessed so far\n",
      "2900 proccessed so far\n",
      "3000 proccessed so far\n",
      "3100 proccessed so far\n",
      "3200 proccessed so far\n",
      "3300 proccessed so far\n",
      "3400 proccessed so far\n",
      "3500 proccessed so far\n",
      "3600 proccessed so far\n",
      "3700 proccessed so far\n",
      "3800 proccessed so far\n",
      "3900 proccessed so far\n",
      "4000 proccessed so far\n",
      "4100 proccessed so far\n",
      "4200 proccessed so far\n",
      "4300 proccessed so far\n",
      "4400 proccessed so far\n",
      "4500 proccessed so far\n",
      "4600 proccessed so far\n",
      "4700 proccessed so far\n",
      "4800 proccessed so far\n",
      "4900 proccessed so far\n",
      "5000 proccessed so far\n",
      "5100 proccessed so far\n",
      "5200 proccessed so far\n",
      "5300 proccessed so far\n",
      "5400 proccessed so far\n",
      "5500 proccessed so far\n",
      "5600 proccessed so far\n",
      "5700 proccessed so far\n",
      "5800 proccessed so far\n",
      "5900 proccessed so far\n",
      "6000 proccessed so far\n",
      "6100 proccessed so far\n",
      "6200 proccessed so far\n",
      "6300 proccessed so far\n",
      "6400 proccessed so far\n",
      "6500 proccessed so far\n",
      "6600 proccessed so far\n",
      "6700 proccessed so far\n",
      "6800 proccessed so far\n",
      "6900 proccessed so far\n",
      "7000 proccessed so far\n",
      "7100 proccessed so far\n",
      "7200 proccessed so far\n",
      "7300 proccessed so far\n",
      "7400 proccessed so far\n",
      "7500 proccessed so far\n",
      "7600 proccessed so far\n",
      "7700 proccessed so far\n",
      "7800 proccessed so far\n",
      "7900 proccessed so far\n",
      "8000 proccessed so far\n",
      "8100 proccessed so far\n",
      "8200 proccessed so far\n",
      "8300 proccessed so far\n",
      "8400 proccessed so far\n",
      "8500 proccessed so far\n",
      "8600 proccessed so far\n",
      "8700 proccessed so far\n",
      "8800 proccessed so far\n",
      "8900 proccessed so far\n",
      "9000 proccessed so far\n",
      "9100 proccessed so far\n",
      "9200 proccessed so far\n",
      "9300 proccessed so far\n",
      "9400 proccessed so far\n",
      "9500 proccessed so far\n",
      "9600 proccessed so far\n",
      "9700 proccessed so far\n",
      "9800 proccessed so far\n",
      "9900 proccessed so far\n",
      "10000 proccessed so far\n",
      "10100 proccessed so far\n",
      "10200 proccessed so far\n",
      "10300 proccessed so far\n",
      "10400 proccessed so far\n",
      "10500 proccessed so far\n",
      "10600 proccessed so far\n",
      "10700 proccessed so far\n",
      "10800 proccessed so far\n",
      "10900 proccessed so far\n",
      "11000 proccessed so far\n",
      "11100 proccessed so far\n",
      "11200 proccessed so far\n",
      "11300 proccessed so far\n",
      "11400 proccessed so far\n",
      "11500 proccessed so far\n",
      "11600 proccessed so far\n",
      "11700 proccessed so far\n",
      "11800 proccessed so far\n",
      "11900 proccessed so far\n",
      "12000 proccessed so far\n",
      "12100 proccessed so far\n",
      "12200 proccessed so far\n",
      "12300 proccessed so far\n",
      "12400 proccessed so far\n",
      "12500 proccessed so far\n",
      "12600 proccessed so far\n",
      "12700 proccessed so far\n",
      "12800 proccessed so far\n",
      "12900 proccessed so far\n",
      "13000 proccessed so far\n",
      "13100 proccessed so far\n",
      "13200 proccessed so far\n",
      "13300 proccessed so far\n",
      "13400 proccessed so far\n",
      "13500 proccessed so far\n",
      "13600 proccessed so far\n",
      "13700 proccessed so far\n",
      "13800 proccessed so far\n",
      "13900 proccessed so far\n",
      "14000 proccessed so far\n",
      "14100 proccessed so far\n",
      "14200 proccessed so far\n",
      "14300 proccessed so far\n",
      "14400 proccessed so far\n",
      "14500 proccessed so far\n",
      "14600 proccessed so far\n",
      "14700 proccessed so far\n",
      "14800 proccessed so far\n",
      "14900 proccessed so far\n",
      "15000 proccessed so far\n",
      "15100 proccessed so far\n",
      "15200 proccessed so far\n",
      "15300 proccessed so far\n",
      "15400 proccessed so far\n",
      "15500 proccessed so far\n",
      "15600 proccessed so far\n",
      "15700 proccessed so far\n",
      "15800 proccessed so far\n",
      "15900 proccessed so far\n",
      "16000 proccessed so far\n",
      "16100 proccessed so far\n",
      "16200 proccessed so far\n",
      "16300 proccessed so far\n",
      "16400 proccessed so far\n",
      "16500 proccessed so far\n",
      "16600 proccessed so far\n",
      "16700 proccessed so far\n",
      "16800 proccessed so far\n",
      "16900 proccessed so far\n",
      "17000 proccessed so far\n",
      "17100 proccessed so far\n",
      "17200 proccessed so far\n",
      "17300 proccessed so far\n",
      "17400 proccessed so far\n",
      "17500 proccessed so far\n",
      "17600 proccessed so far\n",
      "17700 proccessed so far\n",
      "17800 proccessed so far\n",
      "17900 proccessed so far\n",
      "Created in ../input/stage_1_train_images/tf_records/train.record\n",
      "Wall time: 14min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# To overwrite use force=True\n",
    "force = False\n",
    "base_path = '../input/stage_1_train_images/tf_records/'\n",
    "\n",
    "# Load info from csv to a dictionary\n",
    "train_images_metadata = parse_data(train_df)\n",
    "test_images_metadata = parse_data(test_df)\n",
    "\n",
    "# Create TF records in disk\n",
    "create_tf_records(test_images_metadata, base_path, 'validation.record', force=force)\n",
    "create_tf_records(train_images_metadata, base_path, 'train.record', force=force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "It seems like the object detection api have some bugs regarding to Python 3. We have to do these changes in the models repo to make this work with Python 3.\n",
    "\n",
    "--------------\n",
    "\n",
    "Go to `research/object_detection/inference/detection_inference.py` and change \n",
    "\n",
    "`with tf.gfile.Open(inference_graph_path, 'r')`  to \n",
    "\n",
    "`\"with tf.gfile.Open(inference_graph_path, 'rb') as graph_def_file:`\n",
    "\n",
    "This is waiting to be fixed [here](https://github.com/tensorflow/models/pull/5065)\n",
    "\n",
    "--------------\n",
    "\n",
    "\n",
    "Go to `research/object_detection/metrics/tf_example_parser.py` in the parser method of the StringParser class change and change\n",
    "\n",
    "`return \"\".join(tf_example.features.feature[self.field_name]` to\n",
    "\n",
    "`return b\"\".join(tf_example.features.feature[self.field_name]`\n",
    "\n",
    "--------------\n",
    "\n",
    "Go to `research/object_detection/utils/object_detection_evaluation.py` in object_detection_evaluation add this in line 476: \n",
    "`groundtruth_dict[standard_fields.InputDataFields.groundtruth_group_of] != None and`\n",
    "\n",
    "--------------\n",
    "\n",
    "Go to `research/object_detection/utils/object_detection_evaluation.py` and change\n",
    "\n",
    "`category_name = unicode(category_name, 'utf-8')` to\n",
    "\n",
    "`category_name = str(category_name, 'utf-8')`\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train the model we have to call the `train.py` script with the train directory and with the pipeline.config file. The Pipeline config file defines parameters for training/optimization and feature extraction. It also defines the paths of the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python train.py --logtostderr --train_dir=models\\faster_rcnn_resnet50_coco_2018_01_28\\train_dir --pipeline_config_path=models\\faster_rcnn_resnet50_coco_2018_01_28\\pipeline.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we have to create the inference graph to make inferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path models\\faster_rcnn_resnet50_coco_2018_01_28\\pipeline.config --trained_checkpoint_prefix models\\faster_rcnn_resnet50_coco_2018_01_28\\train_dir\\model.ckpt-0 --output_directory models\\faster_rcnn_resnet50_coco_2018_01_28\\fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the inference graph to compute inferences in the validation set. You'll need to install pycocotools:\n",
    "\n",
    "On Linux, run pip install git+https://github.com/waleedka/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\n",
    "\n",
    "On Windows, run pip install git+https://github.com/philferriere/cocoapi.git#egg=pycocotools^&subdirectory=PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python -m infer_detections --input_tfrecord_paths=../input/stage_1_train_images/tf_records/validation.record --output_tfrecord_path=models\\faster_rcnn_resnet50_coco_2018_01_28\\inference --inference_graph=models\\faster_rcnn_resnet50_coco_2018_01_28\\fine_tuned_model\\frozen_inference_graph.pb --discard_image_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Run Metrics against validation set\n",
    "\n",
    "This will compute the open image v2 detection metric against the validation set.\n",
    "(https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/evaluation_protocols.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python offline_eval_map_corloc.py --eval_dir=models\\faster_rcnn_resnet50_coco_2018_01_28\\validation_eval_metrics --eval_config_path=models\\faster_rcnn_resnet50_coco_2018_01_28\\validation_eval_metrics\\validation_eval_config.pbtxt --input_config_path=models\\faster_rcnn_resnet50_coco_2018_01_28\\validation_eval_metrics\\validation_input_config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   OpenImagesV2_Precision/mAP@0.5IOU  0.0\n",
      "0  OpenImagesV2_PerformanceByCategory/AP@0.5IOU/b...  0.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('models/faster_rcnn_resnet50_coco_2018_01_28/validation_eval_metrics/metrics.csv')\n",
    "print(df['OpenImagesV2_Precision/mAP@0.5IOU'][0])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing RSNA score\n",
    "\n",
    "Finally, we'll use the inferences of the validation set to compute the RSNA score that is used in the Kaggle leader board.\n",
    "Interestingly, there is **a lot** of sampes without boxes to predict, these were not added because the Target column was = 0, which means no pneumonia. \n",
    "\n",
    "If we use the samples without pneumonia to compute the scores we got a pretty high score (~0.7), but this is because the implementation of the RSNA score gives 1.0 if both the prediction and the dataset have an empty list of bounding boxes. This is a sort of ambigous case, it needs to be defined, otherwise it will produce a division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Processing file: C:/Users/bones/kaggle/rsna/src/models/faster_rcnn_resnet50_coco_2018_01_28/inference\n",
      "INFO:tensorflow:Processed 0 images...\n",
      "INFO:tensorflow:Processed 1000 images...\n",
      "INFO:tensorflow:Processed 2000 images...\n",
      "INFO:tensorflow:Processed 3000 images...\n",
      "INFO:tensorflow:Processed 4000 images...\n",
      "INFO:tensorflow:Processed 5000 images...\n",
      "INFO:tensorflow:Processed 6000 images...\n",
      "INFO:tensorflow:Processed 7000 images...\n",
      "INFO:tensorflow:Processed 8000 images...\n",
      "INFO:tensorflow:Processed 9000 images...\n"
     ]
    }
   ],
   "source": [
    "configs = config_util.get_configs_from_multiple_files(\n",
    "  eval_input_config_path=\"models/faster_rcnn_resnet50_coco_2018_01_28/validation_eval_metrics/validation_input_config.pbtxt\",\n",
    "  eval_config_path=\"models/faster_rcnn_resnet50_coco_2018_01_28/validation_eval_metrics/validation_eval_config.pbtxt\")\n",
    "\n",
    "eval_config = configs['eval_config']\n",
    "input_config = configs['eval_input_configs'][0]\n",
    "\n",
    "# Set to true to include images from the dataset where there are no bounding boxes to predict\n",
    "count_empty_ground_truth = True\n",
    "\n",
    "input_paths = input_config.tf_record_input_reader.input_path\n",
    "\n",
    "categories = label_map_util.create_categories_from_labelmap(\n",
    "    input_config.label_map_path)\n",
    "\n",
    "skipped_images = 0\n",
    "processed_images = 0\n",
    "\n",
    "predictions = []\n",
    "confidences = []\n",
    "ground_truth = []\n",
    "\n",
    "for input_path in _generate_filenames(input_paths):\n",
    "    tf.logging.info('Processing file: {0}'.format(input_path))\n",
    "    \n",
    "    record_iterator = tf.python_io.tf_record_iterator(path=input_path)\n",
    "    data_parser = tf_example_parser.TfExampleDetectionAndGTParser()\n",
    "    \n",
    "    for string_record in record_iterator:\n",
    "\n",
    "        tf.logging.log_every_n(tf.logging.INFO, 'Processed %d images...', 1000, processed_images)\n",
    "        processed_images += 1\n",
    "\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(string_record)\n",
    "        decoded_dict = data_parser.parse(example)     \n",
    "        \n",
    "        if decoded_dict:\n",
    "            if not count_empty_ground_truth and len(decoded_dict['groundtruth_boxes']) == 0:\n",
    "                continue\n",
    "                \n",
    "            record_confidences = np.array(decoded_dict['detection_scores']).reshape(-1,)\n",
    "            records_ground_truths = []\n",
    "            record_predictions = []\n",
    "\n",
    "            for x1, y1, x2, y2 in decoded_dict['detection_boxes']:\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                assert(w >= 0 and h >= 0)\n",
    "\n",
    "                record_predictions.append([x1, y1, w, h])\n",
    "\n",
    "            for x1, y1, x2, y2 in decoded_dict['groundtruth_boxes']:\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                assert(w >= 0 and h >= 0)\n",
    "\n",
    "                records_ground_truths.append([x1, y1, w, h])\n",
    "            \n",
    "            ground_truth.append(np.array(records_ground_truths))\n",
    "            predictions.append(np.array(record_predictions))\n",
    "            confidences.append(record_confidences)\n",
    "            \n",
    "        else:\n",
    "            skipped_images += 1\n",
    "            tf.logging.info('Skipped images: {0}'.format(skipped_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7184064438881027\n"
     ]
    }
   ],
   "source": [
    "print(utils.mean_rsna_metric(predictions, confidences, ground_truth))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
